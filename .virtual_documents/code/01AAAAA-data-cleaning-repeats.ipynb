











# imports
import pandas as pd
import numpy as np





# reading in shelter intakes
intakes =  pd.read_csv('../data/austin_animal_center_intakes_20241017.csv')
intakes.head()


intakes.info()





# dropping inconsequential columns
intakes.drop(columns=['Name', 'MonthYear'], inplace=True)


# renaming columns to be intake specific and snake case
columns = {
    'Animal ID': 'animal_id',
    'DateTime': 'intake_time',
    'Found Location': 'found_location',
    'Intake Type': 'intake_type',
    'Intake Condition': 'intake_condition',
    'Animal Type': 'animal_type',
    'Sex upon Intake': 'intake_gender',
    'Age upon Intake': 'intake_age',
    'Breed': 'intake_breed',
    'Color': 'intake_color'    
}

intakes = intakes.rename(columns=columns)


# converting intake_time column to datetime format
intakes['intake_time'] = pd.to_datetime(intakes['intake_time'], format='%m/%d/%Y %I:%M:%S %p').dt.normalize()


intakes.nunique()





# Creating a dataframe for repeat animals
repeat_intakes = intakes[intakes.duplicated(subset=['animal_id'], keep=False)].sort_values(by=['animal_id'])
f'There are {len(repeat_intakes)} animals that have made repeated intakes in Austin Animal Shelters.'





# reading in shelter outcomes
outcomes = pd.read_csv('../data/austin_animal_center_outcomes_20241017.csv')
outcomes.head()


outcomes.info()





# dropping inconsequential columns
outcomes.drop(columns=['Name', 'MonthYear', 'Outcome Subtype'], inplace=True)


# renaming columns to be outcome specific and snake case
outcome_columns = {
    'Animal ID': 'animal_id',
    'DateTime': 'outcome_time',
    'Date of Birth': 'date_of_birth',
    'Outcome Type': 'outcome_type',
    'Animal Type': 'outcome_animal_type',
    'Sex upon Outcome': 'outcome_gender',
    'Age upon Outcome': 'outcome_age',
    'Breed': 'outcome_breed',
    'Color': 'outcome_color'    
}

outcomes = outcomes.rename(columns=outcome_columns)


# converting outcome_time and date of birth columns to datetime format
outcomes['outcome_time'] = pd.to_datetime(outcomes['outcome_time'], format='%m/%d/%Y %I:%M:%S %p').dt.normalize()
outcomes['date_of_birth'] = pd.to_datetime(outcomes['date_of_birth'], format='%m/%d/%Y')


outcomes.nunique()





repeat_outcomes = outcomes[outcomes.duplicated(subset=['animal_id'], keep=False)].sort_values(by=['animal_id'])
f'There are {len(repeat_outcomes)} animals that have made repeated outcomes in Austin Animal Shelters.'





# The two dataframes are concat together.  This allows to sort all observations sequentially.
# A column is added to create a date column.
df = pd.concat([repeat_intakes, repeat_outcomes])
df['sequential_date'] = df['intake_time']
df['sequential_date'] = df['sequential_date'].fillna(df['outcome_time'])

df.sort_values(by=['animal_id','sequential_date'], inplace=True)
df.head(10)


df.tail()


# creating a df for intakes and dropping empty intake columns created on the concat
df1_index = df[df['intake_time'].isna()].index
df1 = df.drop(labels=df1_index)
df1.dropna(axis=1, inplace=True)


# creating a unique identifier, animal_stay, to tie with outcomes
df1.sort_values(by=['animal_id', 'sequential_date'], inplace=True)
df1['stay'] = df1.groupby('animal_id').cumcount() +1 
df1['stay'] = df1['stay'].astype('str')
df1['animal_stay'] = df1['animal_id'] + '-' + df1['stay']
df1.shape


# dropping empty outcome columns created on the concat
df2_index = df[df['outcome_time'].isna()].index
df2 = df.drop(labels=df2_index)
df2.dropna(axis=1, inplace=True)


# creating a unique identifier, animal_stay, to tie with intakes
df2.sort_values(by=['animal_id', 'sequential_date'], inplace=True)
df2['stay'] = df2.groupby('animal_id').cumcount() +1 
df2['stay'] = df2['stay'].astype('str')
df2['animal_stay'] = df2['animal_id'] + '-' + df2['stay']
df2.shape


# merging the two dataframes
df3 = pd.merge(left=df2, right=df1, how='inner', on='animal_stay')


# calculate the duration in the shelter
df3['stay_duration'] = df3['outcome_time'] - df3['intake_time']

# convert stay_duration to number of days
df3['stay_duration'] = df3['stay_duration'].dt.days.astype(int)

# check for negative stay_duration
df3[df3['stay_duration'] < 0]['stay_duration'].sort_values()


df3[df3['stay_duration'] == -1]['stay_duration'].sort_values()


df3.shape





# change -1 day stay durations to 0
df3['stay_duration'] = df3['stay_duration'].map(lambda x: 0 if x == -1 else x)

# drop observations with a negative stay duration
print(df3.shape)
df3 = df3[df3['stay_duration'] >= 0]
print(df3.shape)


# Calculate upper boundary
upper_boundary = df3['stay_duration'].quantile(0.75) + (1.5 * (df3['stay_duration'].quantile(0.75) - df3['stay_duration'].quantile(0.25)))





# drop observations with a stay duration excedding 365
print(df3.shape)
df3 = df3[df3['stay_duration'] <= 365]
print(df3.shape)


# compare intake/outcome animal_type, gender, breed, and color
print(f'Number of animal type changes: {df3[df3['animal_type'] != df3['outcome_animal_type']].shape[0]}')
print(f'Number of neuters/spays: {df3[df3['intake_gender'] != df3['outcome_gender']].shape[0]}')
print(f'Number of breed changes: {df3[df3['intake_breed'] != df3['outcome_breed']].shape[0]}')
print(f'Number of color changes: {df3[df3['intake_color'] != df3['outcome_color']].shape[0]}')





# dropping duplicate columns
df3.drop(columns=['outcome_animal_type', 'outcome_breed', 'outcome_color'], inplace=True)

# renaming columns without intake specifier
df3.rename(columns={'intake_breed': 'breed', 'intake_color': 'color', 'animal_id_x': 'animal_id'}, inplace=True)


df3['spay_neuter'] = (df3['intake_gender'] != df3['outcome_gender']).astype(int)


# checking remaining null values
df3.isnull().sum()


# function for converting age columns to age in months
def convert_age(age): 
    '''
    Convert an age in string to age in months.
   
    Argument:
    age(str): The animal age in str format, eg. '7 years'.
   
    Return:
    float: The age converted to months. Return 0 if the unit is not list int the fucntion.
    '''
    value, unit = age.split()
    value = abs(int(value)) # assume the nagetive age is typo 
    
    if 'year' in unit:
        return value * 12
    elif 'month' in unit:
        return value
    elif 'week' in unit:
        return round(float(value * 0.23), 2)
    elif 'day' in unit:
        return round(float(value * 0.033), 2)
    else:
        return 0


df3['intake_age'] = df3['intake_age'].map(convert_age)
df3['outcome_age'] = df3['outcome_age'].map(convert_age)


# checking animal types
df3['animal_type'].value_counts(normalize=True)


# see breeds under Other animal type
df3[df3['animal_type'] == 'Other']['breed'].unique()





# dropping obsevations that aren't cats or dogs
print(df3.shape)
df3 = df3[df3['animal_type'] != 'Other']
df3.shape


df3['repeat'] = 1


# review remaining columns
df3.columns


# Dropping duplicate columns and no longer needed
df3.drop(columns=['animal_id_y', 'sequential_date_x', 'sequential_date_y', 'stay_x', 'stay_y'], inplace=True)


# final check of column names
df3.columns


# saving combined data to use in other notebooks
df3.to_csv('../data/austin-repeats-combined-shelter-data.csv', index=False)








# reading in data for 2014-2015
dallas_2014 = pd.read_csv('../data/Dallas_Animal_Shelter_Data_Fiscal_Year_2014_-_2015_20241028.csv')


pd.set_option('display.max_columns', 35)
dallas_2014.head()





# function to convert column names
def to_snake_case(columns):
    '''
    Convert dataframe column names to snake case

    Keyword arguments:
    columns -- original column names of dataframe

    Returns a dictionary to pass into pandas rename function
    '''
    return {column: column.lower().replace(' ', '_') for column in columns}


dallas_2014.rename(columns = to_snake_case(dallas_2014.columns), inplace = True)


# converting intake_date and outcome_date to datetime
dallas_2014['intake_date'] = pd.to_datetime(dallas_2014['intake_date'], format='%m/%d/%Y %I:%M:%S %p')
dallas_2014['outcome_date'] = pd.to_datetime(dallas_2014['outcome_date'], format='%m/%d/%Y %I:%M:%S %p')

# sorting by outcome_date with most recent first
dallas_2014.sort_values(by = 'outcome_date', ascending = False, inplace = True)





# function to read in data from remainging years
def read_in_dallas_data(year):
    '''
    Function to read in csv data for animal shelter data by year

    Keyword arguments:
    year -- which year to bring in data from

    Returns dataframe with column names converted to snake case,
    intake and outcome dates converted to datetime,
    and sorted by outcome date descending.
    '''
    year = str(year)
    # read in data
    if year != '2023':
        df = pd.read_csv('../data/Dallas_Animal_Shelter_Data_Fiscal_Year_'+year+'_-_'+str(int(year)+1)+'_20241028.csv', low_memory=False)
    else:
        df = pd.read_csv('../data/Dallas_Animal_Shelter_Data_Fiscal_Year_'+year+'_-_'+str(int(year)+2)+'_20241028.csv', low_memory=False)
    # convert column names
    df.rename(columns=to_snake_case(df.columns), inplace = True)
    # convert intake and outcome dates to datetime
    df['intake_date'] = pd.to_datetime(df['intake_date'], format='mixed')
    df['outcome_date'] = pd.to_datetime(df['outcome_date'], format='mixed')
    # sort by outcome_date
    df.sort_values(by = 'outcome_date', ascending = False, inplace = True)
    return df


dallas_2015 = read_in_dallas_data(2015)
dallas_2016 = read_in_dallas_data(2016)
dallas_2017 = read_in_dallas_data(2017)
dallas_2018 = read_in_dallas_data(2018)
dallas_2019 = read_in_dallas_data(2019)
dallas_2020 = read_in_dallas_data(2020)
dallas_2021 = read_in_dallas_data(2021)
dallas_2022 = read_in_dallas_data(2022)
dallas_2023 = read_in_dallas_data(2023)


dallas_combined = pd.concat([dallas_2014, dallas_2015, dallas_2016, dallas_2017, dallas_2018,
                             dallas_2019, dallas_2020, dallas_2021, dallas_2022, dallas_2023])


# resorting combined dataframe by outcome date
dallas_combined.sort_values(by = 'outcome_date', ascending = False, inplace = True)

# dropping duplicates by animal_id to focus on most recent observation per unique animal
print(dallas_combined.shape)
#dallas_combined.drop_duplicates(subset = 'animal_id', inplace = True)
print(dallas_combined.shape)
#dallas_combined.head()


dallas_repeats = dallas_combined[dallas_combined['animal_id'].duplicated()]
dallas_repeats['intake_type'].value_counts(normalize=True)
#dallas_repeats.shape


dallas_repeats


# dropping duplicates by animal_id to focus on most recent observation per unique animal
print(dallas_combined.shape)
dallas_combined.drop_duplicates(subset = 'animal_id', inplace = True)
print(dallas_combined.shape)
#dallas_combined.head()


dallas_combined.info()





dallas_combined.drop(columns = ['kennel_number', 'tag_type', 'activity_number', 'activity_sequence',
                                'source_id', 'census_tract', 'council_district', 'intake_subtype',
                                'intake_total', 'staff_id', 'intake_time', 'due_out', 'hold_request',
                                'outcome_time', 'receipt_number', 'impound_number', 'service_request_number',
                                'additional_information', 'month', 'year', 'outcome_subtype'], inplace = True)


# see animal types
dallas_combined['animal_id'].duplicated()





# see remainging null values
dallas_combined.isnull().sum()





# drop null outcome date rows
dallas_combined.dropna(subset = 'outcome_date', inplace = True)

# column for stay duration
dallas_combined['stay_duration'] = dallas_combined['outcome_date'] - dallas_combined['intake_date']
# convert to number of days
dallas_combined['stay_duration'] = dallas_combined['stay_duration'].dt.days.astype(int)

# check for negative stay_duration
dallas_combined[dallas_combined['stay_duration'] < 0]['stay_duration'].value_counts()





dallas_combined = dallas_combined[dallas_combined['stay_duration'] >= 0]





dallas_combined['reason'].unique()


dallas_combined['outcome_condition'].unique()


dallas_combined['chip_status'].value_counts()


dallas_combined['animal_origin'].unique()





# drop chip_status and animal_origin columns
dallas_combined = dallas_combined.drop(columns=['chip_status', 'animal_origin'])

# fill nulls for reason and outcome_condition
dallas_combined = dallas_combined.fillna({'reason': 'NONE', 'outcome_condition': 'HEALTHY'})


dallas_combined.info()


# saving combined data
dallas_combined.to_csv('../data/dallas-combined-shelter-data.csv', index=False)
